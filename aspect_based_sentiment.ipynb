{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1ef1cf1-db70-4b60-bc9b-17a71009f62f",
   "metadata": {},
   "source": [
    "# Aspect Based Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156d1e67-9d1a-46f1-9af3-9b2a7357eef1",
   "metadata": {},
   "source": [
    "Although we calculated a sentiment score for each quote in our dataset using the built in Sentiment Analysis Analyzer, we did not get good results. Most of our quotes received a neutral score. Those we analyzed that received a high positive or negative score, we often disagreed with. The reason for this is because we are dealing with a very negative topic: police brutality. We hoped the sentiment analysis woud allow us to see what kinds of people are for or against the movement. However, a quote with a very negative score could be negative because it is critical of the police/angry about police brutality or it could be from someone criticizing the movement. Thus, the sentiment scores tell us nothing.\n",
    "\n",
    "Thus, we attempt to create our own aspect based sentiment analysis (ABSA). ABSA allows many subjects and their corresponding sentiments to be found in a given quote/sentence/etc. Research on this is very new and ongoing, so there is no easy was to do it. We go through some steps to see what we can achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3994070-946c-44a6-bf8c-05ccc74a1a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import many libraries\n",
    "import nltk\n",
    "import spacy\n",
    "import stanza\n",
    "import stanfordnlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from spacy import displacy\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.matcher import DependencyMatcher\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6c83c8b-0b10-4c39-8f60-14d0711fee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we read our data since eventually we want to test our ABSA on it \n",
    "df = pd.read_pickle('generated/total-data-merged.pkl.bz2', compression = 'bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e7568b6-866f-46bf-8a7f-a7f6dfc9431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific keywords that we know if mentioned must relate to the Black Lives Matter movement\n",
    "specific_keywords = ['alicia garza','all lives matter','alton sterling','anthony hill',\n",
    "                     'black lives matter','blm','blue lives matter','campaign zero',\n",
    "                     'eric garner','freddie gray','george zimmerman',\"hands up, don't shoot\",\n",
    "                     'movement for black lives','no justice, no peace','patrisse cullors',\n",
    "                     'philando castile','sandra bland','say her name','stop killing us',\n",
    "                     'tamir rice','trayvon martin','unarmed black man','white lives matter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d3c5999-87eb-4bf5-a4da-e5963ee68a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_specific = df[df.quotation.str.contains('|'.join(specific_keywords), case = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19f17af2-23c8-47a0-881d-20afecb9c45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-17 22:28:38 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2021-12-17 22:28:38 INFO: Use device: cpu\n",
      "2021-12-17 22:28:38 INFO: Loading: tokenize\n",
      "2021-12-17 22:28:38 INFO: Loading: pos\n",
      "2021-12-17 22:28:38 INFO: Loading: lemma\n",
      "2021-12-17 22:28:38 INFO: Loading: depparse\n",
      "2021-12-17 22:28:39 INFO: Loading: sentiment\n",
      "2021-12-17 22:28:39 INFO: Loading: constituency\n",
      "2021-12-17 22:28:39 INFO: Loading: ner\n",
      "2021-12-17 22:28:40 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# initializing spacy nlp\n",
    "nlp_spacy = spacy.load('en')\n",
    "nlp_stanza = stanza.Pipeline('en')\n",
    "all_stopwords = nlp_spacy.Defaults.stop_words\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f7e3c8-0463-4cbc-b3a5-87bd41f32694",
   "metadata": {},
   "source": [
    "Our idea: Using natural language processing tools and packages, we input a quote and extract all the subjects from it. Since some subjects can be compounds of multiple words (e.g. Black Lives Matter, police brutality, etc.) we first go through and combine any sets of nouns or proper nouns that are consecutive and combine into one token. This way we get the true subject of the chunk of a sentence and all descriptive words will thus refer to it. We also combine entities into one token. Then, we use dependency parsing to find all the words that describe our noun and separate each quote into a set of nouns with their descriptions. Finally, we calculate the sentiment of each element in the set. Thus if police are described negatively and BLM movement positively, rather than getting a net neutral score we will get two scores and really know the speakers views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f53c9f80-5253-43cf-83be-e00d6c8aba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is our first attempt at doing the whole process\n",
    "\n",
    "def aspect_sentiment_analysis(txt, stop_words=set(stopwords.words('english')), nlp = nlp_stanza):\n",
    "    \n",
    "    #txt = txt.lower() # LowerCasing the given Text\n",
    "    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\n",
    "\n",
    "    fcluster = []\n",
    "    total_feature_list = []\n",
    "    finalcluster = []\n",
    "    dic = {}\n",
    "\n",
    "    for line in sentList:\n",
    "        new_tagged_list = []\n",
    "        txt_list = nltk.word_tokenize(line) # Splitting up into words\n",
    "        tagged_list = nltk.pos_tag(txt_list) # Doing Part-of-Speech Tagging to each word\n",
    "\n",
    "        new_word_list = []\n",
    "        flag = 0\n",
    "        for i in range(0,len(tagged_list)-1):\n",
    "            if(tagged_list[i][1]==\"NN\" and tagged_list[i+1][1]==\"NN\")\\\n",
    "            or (tagged_list[i][1]==\"PROPN\" and tagged_list[i+1][1]==\"PROPN\"): # If two consecutive words are Nouns then they are joined together\n",
    "                new_word_list.append(tagged_list[i][0]+tagged_list[i+1][0])\n",
    "                flag=1\n",
    "            else:\n",
    "                if(flag==1):\n",
    "                    flag=0\n",
    "                    continue\n",
    "                new_word_list.append(tagged_list[i][0])\n",
    "                if(i==len(tagged_list)-2):\n",
    "                    new_word_list.append(tagged_list[i+1][0])\n",
    "\n",
    "        finaltxt = ' '.join(word for word in new_word_list) \n",
    "        new_txt_list = nltk.word_tokenize(finaltxt)\n",
    "        print(new_txt_list)\n",
    "        words_list = [w for w in new_txt_list if not w in stop_words]\n",
    "        tagged_list = nltk.pos_tag(words_list)\n",
    "\n",
    "        doc = nlp(finaltxt) # Object of Stanford NLP Pipeleine\n",
    "        \n",
    "        # Getting the dependency relations betwwen the words\n",
    "        dep_node = []\n",
    "        for dep_edge in doc.sentences[0].dependencies:\n",
    "            dep_node.append([dep_edge[2].text, dep_edge[0].id, dep_edge[1]])\n",
    "\n",
    "        # Coverting it into appropriate format\n",
    "        for i in range(0, len(dep_node)):\n",
    "            if (int(dep_node[i][1]) != 0):\n",
    "                dep_node[i][1] = new_word_list[(int(dep_node[i][1]) - 1)]\n",
    "\n",
    "        feature_list = []\n",
    "        categories = []\n",
    "        for i in tagged_list:\n",
    "            if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n",
    "                feature_list.append(list(i)) # For features for each sentence\n",
    "                total_feature_list.append(list(i)) # Stores the features of all the sentences in the text\n",
    "                categories.append(i[0])\n",
    "\n",
    "        for i in feature_list:\n",
    "            filist = []\n",
    "            for j in dep_node:\n",
    "                if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "                    if(j[0]==i[0]):\n",
    "                        filist.append(j[1])\n",
    "                    else:\n",
    "                        filist.append(j[0])\n",
    "            fcluster.append([i[0], filist])\n",
    "    \n",
    "    sentiments = []\n",
    "    for i in total_feature_list:\n",
    "        dic[i[0]] = i[1]\n",
    "        \n",
    "    for i in fcluster:\n",
    "        if(dic[i[0]]==\"NN\"):\n",
    "            sentiment = []\n",
    "            for aspect in i[1]:\n",
    "                sent = TextBlob(aspect).sentiment[0]\n",
    "                sentiment.append(sent)\n",
    "            sentiments.append(sentiment)\n",
    "            finalcluster.append(i)\n",
    "        \n",
    "    return(finalcluster,sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc07f849-c0fc-4cd6-9ae9-f8c1e05f9e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-17 22:33:28 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2021-12-17 22:33:28 INFO: Use device: cpu\n",
      "2021-12-17 22:33:28 INFO: Loading: tokenize\n",
      "2021-12-17 22:33:28 INFO: Loading: pos\n",
      "2021-12-17 22:33:29 INFO: Loading: lemma\n",
      "2021-12-17 22:33:29 INFO: Loading: depparse\n",
      "2021-12-17 22:33:29 INFO: Loading: sentiment\n",
      "2021-12-17 22:33:30 INFO: Loading: constituency\n",
      "2021-12-17 22:33:31 INFO: Loading: ner\n",
      "2021-12-17 22:33:31 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'soundquality', 'is', 'great', 'but', 'the', 'batterylife', 'is', 'very', 'bad', '.']\n",
      "([['soundquality', ['great']], ['batterylife', ['bad']]], [[0.8], [-0.6999999999999998]])\n"
     ]
    }
   ],
   "source": [
    "# running the function on an example and we see that it can extract positive and negative sentiment\n",
    "nlp = stanza.Pipeline('en')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "txt = \"The sound quality is great but the battery life is very bad.\"\n",
    "\n",
    "print(aspect_sentiment_analysis(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51395492-c633-4eed-ad79-67694c68ab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df_specific.sample(30).quotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23953d5-d2fc-4ce2-83af-207bef0f30fe",
   "metadata": {},
   "source": [
    "Here, however it does not work so well and is a bit confusing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f09a904-8987-4c45-b958-99d0e9e7309a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My question for the candidates: Do black lives matter or do all lives matter?\n",
      "['...', 'makes', 'a', 'powerful', 'statement', 'to', 'those', 'of', 'use', 'in', 'lawenforcement', ',', 'and', 'it', \"'s\", 'one', 'that', 'we', 'appreciate', '.']\n",
      "['My', 'philosophy', 'is', 'all', 'lives', 'matter', ',', 'but', 'I', 'hope', 'people', 'realize', 'that', 'those', 'of', 'us', 'in', 'lawenforcement', 'overwhelmingly', 'want', 'to', 'protect', 'those', 'in', 'our', 'communities', '.']\n",
      "([['statement', ['powerful', 'makes']], ['use', []], ['appreciate', ['that', 'we', 'one']], ['philosophy', ['matter']], ['matter', ['philosophy', 'lives']]], [[0.3, 0.0], [], [0.0, 0.0, 0.0], [0.0], [0.0, 0.0]])\n"
     ]
    }
   ],
   "source": [
    "print(sample[2])\n",
    "print(aspect_sentiment_analysis(sample[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11bbcbd9-1186-44f2-9c24-d43c2189b08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that extracts all the subjects from the sentence and makes sure they are one entity.\n",
    "\n",
    "def extract_aspects(doc):\n",
    "    sentences = doc.sents\n",
    "    tokens_and_entities = []\n",
    "    tokens_and_pos = []\n",
    "    \n",
    "    # loop through each sentence\n",
    "    for sent in sentences: \n",
    "        tokens_and_pos = []\n",
    "        entities = [entity.text for entity in sent.ents] # get all entities\n",
    "        tokens = [token.text for token in sent] # get all tokens\n",
    "        num_tokens = len(tokens)\n",
    "        flag = 0\n",
    "        \n",
    "        # loop throug each token and see if it can form a compound subject with consecutive words\n",
    "        for idx, token in enumerate(sent):\n",
    "            for i in range(0, num_tokens):\n",
    "                toks = [tok.text for tok in sent[idx:num_tokens-i]]\n",
    "                parts_of_speech = [tok.pos_ for tok in sent[idx:num_tokens-i]] # extract parts of speech\n",
    "                \n",
    "                if (len(parts_of_speech)) > 1 and (flag==0):\n",
    "                    # check if there are consecutive nouns. if yes, append as a singl new token\n",
    "                    if all(elem == parts_of_speech[0] for elem in parts_of_speech) and (parts_of_speech[0] in [\"NOUN\", \"PROPN\"]):\n",
    "                        tokens_and_pos.append(''.join(toks))\n",
    "                        flag = len(range(idx,num_tokens-i))\n",
    "                        break  \n",
    "                        \n",
    "            # flags to skip the current token if they were already added from compounding with the previous token\n",
    "            if (flag>0):\n",
    "                flag -= 1\n",
    "            else:\n",
    "                tokens_and_pos.append(token.text)\n",
    "                \n",
    "                \n",
    "        print(tokens_and_pos)\n",
    "        flag = 0\n",
    "        # looping through eacch token to see if it forms an entity with consecutive tokens\n",
    "        for idx, token in enumerate(tokens_and_pos):\n",
    "            for i in range(0, num_tokens):\n",
    "                possible_entity = ' '.join(tokens_and_pos[idx:num_tokens-i])\n",
    "                # if an entity was created, append it as a single token\n",
    "                if possible_entity in entities and (flag==0):\n",
    "                    tokens_and_entities.append(''.join(tokens_and_pos[idx:num_tokens-i]))\n",
    "                    flag = len(range(idx,num_tokens-i))\n",
    "                    break    \n",
    "\n",
    "            if (flag>0):\n",
    "                flag -= 1\n",
    "            else:\n",
    "                tokens_and_entities.append(token)\n",
    "                \n",
    "    # we return the final list of tokens\n",
    "    return tokens_and_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a292f8-bf64-4e34-9b35-79f99ec5e401",
   "metadata": {},
   "source": [
    "The function above is a bit more expansive but it only does the first half of the task. We now need to get the descriptions of each token. This is where we fail to make a useful ABSA. Our attempts are shown below. Grammatical structure is complex and acquiring all related descriptions fails at times. With our code, we could not achieve any guarantee that the nouns and their descriptions really correspond to each other. Thus, we can not use this for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f042c1-947c-4e62-8f70-1da78e82853c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here we attempt to get the descriptions of all the subjects in a document and gives the subsets of sentiments\n",
    "for token in doc:\n",
    "    possible_sents = []\n",
    "    if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "        deps = []\n",
    "        sentiments = []\n",
    "        for ancestor in token.ancestors:\n",
    "            if ancestor.pos_ in [\"ADJ\", \"VERB\", \"NOUN\"]:\n",
    "                deps.append(ancestor.text)\n",
    "        for child in token.children:\n",
    "            if child.pos_ in [\"ADJ\", \"VERB\", \"NOUN\"]:\n",
    "                deps.append(child.text)\n",
    "        for d in deps:\n",
    "            sentiments.append(analyzer.polarity_scores(d))\n",
    "            \n",
    "        print(token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3a7a95-d5ae-464a-8615-37643662c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here trying to get noun adjective pairs\n",
    "noun_adj_pairs = []\n",
    "for i,token in enumerate(doc):\n",
    "    if token.pos_ not in ('NOUN','PROPN'):\n",
    "        continue\n",
    "    for j in range(i+1,len(doc)):\n",
    "        if doc[j].pos_ == 'ADJ':\n",
    "            noun_adj_pairs.append((token,doc[j]))\n",
    "            break\n",
    "noun_adj_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fff9cb8-0702-4c54-97e0-9c94c1607d99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
